import torch
import torch.nn as nn

class MHA(nn.Module):
    def __init__(self):
        # NUmber of attention blocks

        # Concat and normalisation

        # Feedforward to compressed state


        pass

    def forward(self):
        pass

class Attention(nn.Module):
    def __init__(self):
        pass

    def forward(self):
        pass

class Transformer(nn.Module):
    pass